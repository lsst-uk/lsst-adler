{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7f7ae6c-3d28-4b5c-8d6a-9fc4d9dda70a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T14:06:02.572156Z",
     "iopub.status.busy": "2025-11-03T14:06:02.571870Z",
     "iopub.status.idle": "2025-11-03T14:06:03.137042Z",
     "shell.execute_reply": "2025-11-03T14:06:03.136222Z",
     "shell.execute_reply.started": "2025-11-03T14:06:02.572133Z"
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5f2e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_filepath = \"/home/astro-sobrien/\"\n",
    "# root_filepath = \"/Volumes/astro-sobrien/home/astro-sobrien/\"\n",
    "# root_filepath = \"/Users/seanobrien/Documents/Adler/\"\n",
    "current_path = os.getcwd()\n",
    "root_filepath = current_path.split('lsst-adler')[0]\n",
    "\n",
    "test_db_filename = f\"{root_filepath}lsst-adler/tests/data/testing_database.db\"\n",
    "rubin_sql_filename_oct = f\"{root_filepath}rubin_251002.sqlite\"\n",
    "rubin_sql_filename_nov = f\"{root_filepath}rubin_251105.sqlite\"\n",
    "rubin_sql_filename_nov2 = f\"{root_filepath}rubin_251111.sqlite\"\n",
    "adler_output_filename = f\"{root_filepath}adler_output_251111_v1.sqlite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a852848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from astropy.time import Time\n",
    "log_timestamp = Time.now().isot.replace(\":\", '_')\n",
    "\n",
    "# --- Reset logging system (important for Jupyter) ---\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# --- Configure root logger to log only to a file ---\n",
    "logging.basicConfig(\n",
    "    filename=f\"{root_filepath}adler_test_{log_timestamp}.log\",\n",
    "    filemode='w',  # 'w' to overwrite each run, 'a' to append\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    level=logging.DEBUG  # capture INFO, DEBUG, etc.\n",
    ")\n",
    "\n",
    "# Optional confirmation\n",
    "logging.getLogger().info(f\"Root logging configured to write to {root_filepath}adler_test{log_timestamp}.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8130acac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from adler.objectdata.AdlerPlanetoid import AdlerPlanetoid\n",
    "\n",
    "from adler.objectdata.Observations import Observations\n",
    "from adler.objectdata.MPCORB import MPCORB\n",
    "from adler.objectdata.SSObject import SSObject\n",
    "from adler.objectdata.AdlerData import AdlerData\n",
    "from adler.objectdata.objectdata_utilities import get_data_table, get_from_table, utc_to_mjd, mjd_to_utc\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88777f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_DEPENDENT_KEYS = [\"phaseAngle_min\", \"phaseAngle_range\", \"nobs\", \"arc\"]\n",
    "MODEL_DEPENDENT_KEYS = [\n",
    "    \"H\",\n",
    "    \"H_err\",\n",
    "    \"phase_parameter_1\",\n",
    "    \"phase_parameter_1_err\",\n",
    "    \"phase_parameter_2\",\n",
    "    \"phase_parameter_2_err\",\n",
    "    \"modelFitMjd\",\n",
    "]\n",
    "ALL_FILTER_LIST = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n",
    "\n",
    "def tmp_get_row_data_and_columns(self):\n",
    "    \"\"\"Collects all of the data present in the AdlerData object as a list with a corresponding list of column names,\n",
    "    in preparation for a row to be written to a SQL database table.\n",
    "\n",
    "    Returns\n",
    "    -----------\n",
    "    row_data : list\n",
    "        A list containing all of the relevant data present in the AdlerData object.\n",
    "\n",
    "    required_columns : list of str\n",
    "        A list of the corresponding column names in the same order.\n",
    "\n",
    "    \"\"\"\n",
    "    required_columns = [\"ssObjectId\", \"timestamp\"]\n",
    "    # edited here to remove int() around ssObjectId\n",
    "    row_data = [self.ssObjectId, Time.now().mjd]\n",
    "\n",
    "    for f, filter_name in enumerate(self.filter_list):\n",
    "        columns_by_filter = [\"_\".join([filter_name, filter_key]) for filter_key in FILTER_DEPENDENT_KEYS]\n",
    "        data_by_filter = [\n",
    "            getattr(self.filter_dependent_values[f], filter_key) for filter_key in FILTER_DEPENDENT_KEYS\n",
    "        ]\n",
    "\n",
    "        required_columns.extend(columns_by_filter)\n",
    "        row_data.extend(data_by_filter)\n",
    "\n",
    "        for m, model_name in enumerate(self.filter_dependent_values[f].model_list):\n",
    "            columns_by_model = [\n",
    "                \"_\".join([filter_name, model_name, model_key]) for model_key in MODEL_DEPENDENT_KEYS\n",
    "            ]\n",
    "            data_by_model = [\n",
    "                getattr(self.filter_dependent_values[f].model_dependent_values[m], model_key)\n",
    "                for model_key in MODEL_DEPENDENT_KEYS\n",
    "            ]\n",
    "\n",
    "            required_columns.extend(columns_by_model)\n",
    "            row_data.extend(data_by_model)\n",
    "\n",
    "    return row_data, required_columns\n",
    "\n",
    "def tmp_get_database_connection(self, filepath, create_new=False):\n",
    "        \"\"\"Returns the connection to the output SQL database, creating it if it does not exist.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        filepath : path-like object\n",
    "            Filepath with the location of the output SQL database.\n",
    "\n",
    "        create_new : Boolean\n",
    "            Whether to create the database if it doesn't already exist. Default is False.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        con : sqlite3 Connection object\n",
    "            The connection to the output database.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        database_exists = os.path.isfile(\n",
    "            filepath\n",
    "        )  # check this FIRST as the next statement creates the db if it doesn't exist\n",
    "\n",
    "        if not database_exists and create_new:  # we need to make the table and a couple of starter columns\n",
    "            con = sqlite3.connect(filepath)\n",
    "            cur = con.cursor()\n",
    "            # Using ssObjectId (which is actually provid) as primary key, canged to TEXT\n",
    "            cur.execute(\"CREATE TABLE AdlerData(ssObjectId TEXT, timestamp REAL, PRIMARY KEY (ssObjectId))\")\n",
    "            # added creation of AdlerFlags table\n",
    "            cur.execute(\"CREATE TABLE AdlerFlags(ssObjectId TEXT, filter_name TEXT, diaSourceId TEXT, midPointMjdTai INTEGER, outlier INTEGER)\")\n",
    "        elif not database_exists and not create_new:\n",
    "            logger.error(\"ValueError: Database cannot be found at given filepath.\")\n",
    "            raise ValueError(\"Database cannot be found at given filepath.\")\n",
    "        else:\n",
    "            con = sqlite3.connect(filepath)\n",
    "\n",
    "        return con\n",
    "\n",
    "AdlerData._get_row_data_and_columns = tmp_get_row_data_and_columns\n",
    "AdlerData._get_database_connection = tmp_get_database_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "143ba02a-cea2-46b4-8188-6f0da0e6ef00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T14:06:03.143365Z",
     "iopub.status.busy": "2025-11-03T14:06:03.143140Z",
     "iopub.status.idle": "2025-11-03T14:06:03.173518Z",
     "shell.execute_reply": "2025-11-03T14:06:03.172875Z",
     "shell.execute_reply.started": "2025-11-03T14:06:03.143347Z"
    }
   },
   "outputs": [],
   "source": [
    "conn_oct = sqlite3.connect(rubin_sql_filename_oct)\n",
    "cur_oct = conn_oct.cursor()\n",
    "\n",
    "conn_nov = sqlite3.connect(rubin_sql_filename_nov)\n",
    "cur_nov = conn_nov.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86c9fa0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1765c9940>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn_adler_out = sqlite3.connect(adler_output_filename)\n",
    "cur_adler_out = conn_adler_out.cursor()\n",
    "\n",
    "cur_adler_out.execute(\"DROP TABLE IF EXISTS AdlerData;\")\n",
    "cur_adler_out.execute(\"DROP TABLE IF EXISTS  AdlerFlags;\")\n",
    "\n",
    "cur_adler_out.execute(\"CREATE TABLE AdlerData(ssObjectId TEXT, timestamp REAL, PRIMARY KEY (ssObjectId))\")\n",
    "# added creation of AdlerFlags table\n",
    "cur_adler_out.execute(\"CREATE TABLE AdlerFlags(ssObjectId TEXT, filter_name TEXT, diaSourceId TEXT, midPointMjdTai INTEGER, outlier INTEGER)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8546cb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First time setup alterations to the newer Rubin sqlite file\n",
    "# # Remove the L before filter names used by the MPC to distinguish the LSST bands\n",
    "# # We do this for consistency with adler and the RSP\n",
    "# cur_nov.execute(\"UPDATE obs_sbn SET band=substr(band, 2) WHERE band LIKE 'L%';\")\n",
    "\n",
    "# #Need to commit changes before we can operate on the database again\n",
    "# conn_nov.commit()\n",
    "\n",
    "# #Rename mpc_orbits to MPCORB for consistency with adler and the RSP\n",
    "# cur_nov.execute(\"ALTER TABLE mpc_orbits RENAME TO MPCORB;\")\n",
    "# conn_nov.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e364622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 MIN(obstime)\n",
      "0  2024-11-24T02:52:28.634000\n",
      "                 MIN(obstime)\n",
      "0  2024-11-24T02:52:28.634000\n",
      "                 MAX(obstime)\n",
      "0  2025-05-05T02:05:32.780000\n",
      "                 MAX(obstime)\n",
      "0  2025-05-05T02:05:32.780000\n",
      "   COUNT(*)\n",
      "0    345989\n",
      "   COUNT(*)\n",
      "0    458132\n",
      "   COUNT(DISTINCT provid)\n",
      "0                    2196\n",
      "   COUNT(DISTINCT provid)\n",
      "0                    2565\n",
      "  band  count(*)\n",
      "0    g    109973\n",
      "1    i     72380\n",
      "2    r    163521\n",
      "3    u       115\n",
      "  band  count(*)\n",
      "0    g    142927\n",
      "1    i     96860\n",
      "2    r    206867\n",
      "3    u     11478\n"
     ]
    }
   ],
   "source": [
    "print(pd.read_sql_query(\"SELECT MIN(obstime) FROM obs_sbn\", conn_oct))\n",
    "print(pd.read_sql_query(\"SELECT MIN(obstime) FROM obs_sbn\", conn_nov))\n",
    "\n",
    "print(pd.read_sql_query(\"SELECT MAX(obstime) FROM obs_sbn\", conn_oct))\n",
    "print(pd.read_sql_query(\"SELECT MAX(obstime) FROM obs_sbn\", conn_nov))\n",
    "\n",
    "print(pd.read_sql_query(\"SELECT COUNT(*) FROM obs_sbn\", conn_oct))\n",
    "print(pd.read_sql_query(\"SELECT COUNT(*) FROM obs_sbn\", conn_nov))\n",
    "\n",
    "print(pd.read_sql_query(\"SELECT COUNT(DISTINCT provid) FROM obs_sbn\", conn_oct))\n",
    "print(pd.read_sql_query(\"SELECT COUNT(DISTINCT provid) FROM obs_sbn\", conn_nov))\n",
    "\n",
    "print(pd.read_sql_query(\"SELECT DISTINCT(band), count(*) FROM obs_sbn GROUP BY band\", conn_oct))\n",
    "print(pd.read_sql_query(\"SELECT DISTINCT(band), count(*) FROM obs_sbn GROUP BY band\", conn_nov))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec6277",
   "metadata": {},
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6b3072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import adler.utilities.science_utilities as sci_utils\n",
    "\n",
    "# Set threshold for magnitude jumps\n",
    "diff_cut = 1.0\n",
    "std_cut = 3.0\n",
    "# Set column to use for magnitude (we don't currently have reduced_mag populated for MPC file format)\n",
    "mag_col = 'mag'\n",
    "magErr_col = 'magErr'\n",
    "# Number of days of data to retrieve (i.e. previous 30 nights)\n",
    "# data_timespan=30\n",
    "# data_timespan=15*365\n",
    "\n",
    "# Set filter list (only ugri present currently, very few u, but keeping in for completeness)\n",
    "filter_list=['u', 'g', 'r', 'i', 'z', 'y']\n",
    "# filter_list=['g', 'r', 'i']\n",
    "\n",
    "#Establish DB connection\n",
    "conn_adler_out = sqlite3.connect(adler_output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b106c7be",
   "metadata": {},
   "source": [
    "## Analysing first obs file in bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2f42914",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_objs_df_oct = pd.read_sql_query(\"SELECT DISTINCT provid FROM obs_sbn\", conn_oct)\n",
    "unique_obj_ids = distinct_objs_df_oct.provid.to_numpy()\n",
    "logger.info(f\"{len(unique_obj_ids)} objects to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6be283c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2196/2196 [08:46<00:00,  4.17it/s]\n"
     ]
    }
   ],
   "source": [
    "make_plots=True\n",
    "\n",
    "for obj_id in tqdm(unique_obj_ids):\n",
    "    #Taking all data, no time constraint\n",
    "    planetoid = AdlerPlanetoid.construct_from_mpc_obs_sbn(ssObjectId=obj_id,\n",
    "                                                        sql_filename=rubin_sql_filename_oct,\n",
    "                                                        filter_list=filter_list)\n",
    "\n",
    "    #First file so we know there isn't anything in the AdlerData table yet so won't try to populate from DB yet\n",
    "    adler_data = AdlerData(obj_id, planetoid.filter_list)\n",
    "\n",
    "    for filt in planetoid.filter_list:\n",
    "        df_obs = sci_utils.get_df_obs_filt(planetoid, filt=filt)\n",
    "\n",
    "        nobs_nomask = len(df_obs)\n",
    "\n",
    "        err_flag = df_obs.magErr.isnull().all()\n",
    "        if err_flag:\n",
    "            logger.info(\"All magErr values are NaNs, proceed with caution\")\n",
    "        else:\n",
    "        \n",
    "            # Remove observations with large errorbars\n",
    "            magErr_percentile_cut = 95    #Value (between 0 and 100) to define the percentile above which we cut data with large magErr values\n",
    "            magErr_mask = (df_obs.magErr <= np.nanpercentile(df_obs.magErr, q=magErr_percentile_cut))\n",
    "            df_obs = df_obs[magErr_mask]\n",
    "        \n",
    "        # First loop so there are no previous outliers\n",
    "        df_obs[\"outlier\"] = [False] * len(df_obs)\n",
    "\n",
    "        median_mag = np.median(df_obs[mag_col])\n",
    "        res = np.array(df_obs[mag_col]) - median_mag\n",
    "        \n",
    "        if err_flag:\n",
    "            logger.info(f\"No measurement errors, using outlier_diff to check for outliers more than {diff_cut} magnitudes away\")\n",
    "            outlier_flag = sci_utils.outlier_diff(res, diff_cut=diff_cut)\n",
    "        else:\n",
    "            logger.info(f\"Measurement errors available, using outlier_sigma_diff to check for outliers more than {std_cut} standard deviation away\")\n",
    "            outlier_flag = sci_utils.outlier_sigma_diff(res, df_obs[magErr_col], std_sigma=std_cut)\n",
    "\n",
    "        logger.info(\"{} outliers detected\".format(np.sum(outlier_flag)))\n",
    "        \n",
    "        # Write only outlier flags\n",
    "        df_obs.loc[:,\"outlier\"] = outlier_flag\n",
    "        df_obs.loc[outlier_flag, ['ssObjectId', 'filter_name', 'diaSourceId', 'midPointMjdTai', 'outlier']].to_sql('AdlerFlags', con=conn_adler_out, if_exists='append', index=False)\n",
    "        logger.info(f\"Flags for {obj_id} written to AdlerFlags table\")\n",
    "\n",
    "        # # Write all flags (True and False)\n",
    "        # df_obs.loc[:,\"outlier\"] = outlier_flag\n",
    "        # df_obs[['ssObjectId', 'filter_name', 'diaSourceId', 'midPointMjdTai', 'outlier']].to_sql('AdlerFlags', con=conn_adler_out, if_exists='append', index=False)\n",
    "        \n",
    "        #Populate nobs parameter\n",
    "        #FIXME Currently using the number of observations in the filter before masking out values with large errorbars\n",
    "        # This is so that the comparison with the new observations file can be made to assess what objects have new observations\n",
    "        # This may need to change (possibly populate a new parameter) as technically this value should be the number of values used in the fit\n",
    "        # But currently the \"fit\" is just taking the median anyway, rather than any kind of phase curve\n",
    "        #P ossible solution is below in the new observations file loop, we could load in the observations for each object and then just continue to next object if no new observations (doing a check on diaSourceId to guard against if weird stuff happens with the masking)\n",
    "        adler_data.populate_phase_parameters(filt, **{'nobs':nobs_nomask})\n",
    "        adler_data.write_row_to_database(adler_output_filename)\n",
    "        \n",
    "        if make_plots:\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.errorbar(df_obs['midPointMjdTai'], df_obs['mag'], df_obs['magErr'], ls='', marker='.')\n",
    "            ax.errorbar(df_obs[outlier_flag]['midPointMjdTai'], df_obs[outlier_flag]['mag'], df_obs[outlier_flag]['magErr'], ls='', marker='.')\n",
    "            ax.axhline(median_mag)\n",
    "            ax.invert_yaxis()\n",
    "            ax.set_xlabel(\"Time [MJD]\")\n",
    "            ax.set_ylabel(f\"{filt}-band Magnitude\")\n",
    "            fig.savefig(f\"plots/{obj_id}_{filt}_outliers.png\", bbox_inches='tight', pad_inches=0.05)\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9f3ec",
   "metadata": {},
   "source": [
    "## Loop for processing new file now that database has been established"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4d8a80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x171cd3ec0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attach the adler output database to the new observations database with sqlite so we can join tables\n",
    "cur_nov.execute(f\"ATTACH DATABASE '{adler_output_filename}' AS adler_db;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6decca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column information to check what filters have previously been analysed (and therefore have {filter}_nobs columns in AdlerData)\n",
    "cur_adler_out.execute(\"PRAGMA table_info(AdlerData);\")\n",
    "adler_out_cols = [row[1] for row in cur_adler_out.fetchall()]\n",
    "filter_nobs_columns = [c for c in adler_out_cols if c.endswith('_nobs')]\n",
    "current_adlerdata_filters = [c.replace('_nobs', '') for c in filter_nobs_columns]\n",
    "\n",
    "# Build CASE section of SQL query that counts observations in each filter for each object\n",
    "query_cases_list = [f\"SUM(CASE WHEN band = '{b}' THEN 1 ELSE 0 END) AS {b}_nobs\" for b in current_adlerdata_filters]\n",
    "current_filters_joined = \"', '\".join(current_adlerdata_filters)\n",
    "# Include a special case that catches any observations in new filters\n",
    "query_cases_list.append(f\"SUM(CASE WHEN band NOT IN ('{current_filters_joined}') THEN 1 ELSE 0 END) AS new_band_count\")\n",
    "cases_sql = \",\\n       \".join(query_cases_list) # Join these all together\n",
    "\n",
    "# Build WHERE clause for SQL query\n",
    "# Check for objects with more observations in filters populated in AdlerData\n",
    "comparison_parts = [\n",
    "    f\"obs_counts.{b}_nobs > adler_data.{b}_nobs\"\n",
    "    for b in current_adlerdata_filters\n",
    "]\n",
    "# Check for objects with observations in filters not previously populated in AdlerData\n",
    "comparison_parts.append(\"obs_counts.new_band_count > 0\")\n",
    "# Check for entirely new objects not previously analzyed\n",
    "comparison_parts.append(\"adler_data.SSObjectId IS NULL\")\n",
    "where_clause = \"\\nOR \".join(comparison_parts) #Join these all together\n",
    "\n",
    "# Create full SQL query\n",
    "new_obj_sql_query = f\"\"\"\n",
    "SELECT obs_counts.provid\n",
    "FROM (\n",
    "    SELECT provid,\n",
    "        {cases_sql}\n",
    "    FROM obs_sbn\n",
    "    GROUP BY provid\n",
    ") AS obs_counts\n",
    "LEFT JOIN AdlerData AS adler_data\n",
    "    ON obs_counts.provid = adler_data.SSObjectId\n",
    "WHERE {where_clause};\n",
    "\"\"\"\n",
    "\n",
    "logger.info(f\"Executing {new_obj_sql_query} on {rubin_sql_filename_nov}\")\n",
    "\n",
    "cur_nov.execute(new_obj_sql_query)\n",
    "# Forcing dtype object here as otherwise the values become np.str_ type, not sure if this is good or bad\n",
    "obj_list = np.array([i[0] for i in cur_nov.fetchall()], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ca2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 369/369 [02:17<00:00,  2.69it/s]\n"
     ]
    }
   ],
   "source": [
    "make_plots=True\n",
    "\n",
    "for obj_id in tqdm(obj_list):\n",
    "    #Set first_analysis flag to False by default (object could have new observations so would still appear in AdlerData/AdlerFlags)\n",
    "    first_analysis = False\n",
    "    #Taking all data, no time constraint\n",
    "    planetoid = AdlerPlanetoid.construct_from_mpc_obs_sbn(ssObjectId=obj_id,\n",
    "                                                        sql_filename=rubin_sql_filename_nov,\n",
    "                                                        filter_list=filter_list)\n",
    "\n",
    "    adler_data = AdlerData(obj_id, planetoid.filter_list)\n",
    "    try:\n",
    "        adler_data.populate_from_database(adler_output_filename)\n",
    "    except ValueError:\n",
    "        logger.info(f\"No AdlerData entry for {obj_id}, proceeding with empty AdlerData object\")\n",
    "        first_analysis = True\n",
    "\n",
    "    for filt in planetoid.filter_list:\n",
    "        df_obs = sci_utils.get_df_obs_filt(planetoid, filt=filt)\n",
    "\n",
    "        nobs_nomask = len(df_obs)\n",
    "\n",
    "        err_flag = df_obs.magErr.isnull().all()\n",
    "        if err_flag:\n",
    "            logger.info(\"All magErr values are NaNs, proceed with caution\")\n",
    "        else:\n",
    "            # Remove observations with large errorbars\n",
    "            magErr_percentile_cut = 95    #Value (between 0 and 100) to define the percentile above which we cut data with large magErr values\n",
    "            magErr_mask = (df_obs.magErr <= np.nanpercentile(df_obs.magErr, q=magErr_percentile_cut))\n",
    "            df_obs = df_obs[magErr_mask]\n",
    "        \n",
    "        # First loop so there are no previous outliers\n",
    "        if first_analysis:\n",
    "            logger.info(\"First analysis set to True so no outlier flags available to load\")\n",
    "            df_obs[\"outlier\"] = [False] * len(df_obs)\n",
    "        else:\n",
    "            logger.info(\"Loading previous outlier flags\")\n",
    "            sql_query = f\"SELECT * FROM AdlerFlags WHERE ssObjectId='{obj_id}' AND filter_name='{filt}'\"\n",
    "            flags_df = pd.read_sql_query(sql_query, conn_adler_out)\n",
    "            logger.info(\"{} previous outliers found\".format(len(flags_df)))\n",
    "\n",
    "            # Include the outlier flags in the observations dataframe\n",
    "            df_obs = df_obs.merge(flags_df[['diaSourceId', 'outlier']],\n",
    "                                on='diaSourceId',\n",
    "                                how='left')\n",
    "            df_obs['outlier'] = df_obs['outlier'].astype('boolean').fillna(False)\n",
    "\n",
    "        \n",
    "        median_mag = np.median(df_obs.loc[~df_obs['outlier'], mag_col])\n",
    "        res = np.array(df_obs[mag_col]) - median_mag\n",
    "        \n",
    "        if err_flag:\n",
    "            logger.info(f\"No measurement errors, using outlier_diff to check for outliers more than {diff_cut} magnitudes away\")\n",
    "            outlier_flag = sci_utils.outlier_diff(res, diff_cut=diff_cut)\n",
    "        else:\n",
    "            logger.info(f\"Measurement errors available, using outlier_sigma_diff to check for outliers more than {std_cut} standard deviation away\")\n",
    "            outlier_flag = sci_utils.outlier_sigma_diff(res, df_obs[magErr_col], std_sigma=std_cut)\n",
    "\n",
    "        logger.info(\"{} outliers detected\".format(np.sum(outlier_flag)))\n",
    "        \n",
    "        # Check if any outliers swapped from True to False and update in AdlerFlags\n",
    "        true_to_false_mask = (df_obs['outlier']) & (~outlier_flag)\n",
    "        logger.info(\"{} previous outliers now determined to be not significant\".format(np.sum(true_to_false_mask)))\n",
    "        if np.sum(true_to_false_mask)>0:\n",
    "            update_query = f\"\"\"UPDATE AdlerFlags SET outlier=0 WHERE diaSourceId IN {df_obs.loc[true_to_false_mask, 'diaSourceId'].to_numpy()}\"\"\"\n",
    "            logger.info(f\"Executing {update_query} on {adler_output_filename}\")\n",
    "            cur_adler_out.execute(update_query)\n",
    "            conn_adler_out.commit()\n",
    "\n",
    "        # Check for new outliers (i.e. have swapped from False to True)\n",
    "        false_to_true_mask = (~df_obs['outlier']) & (outlier_flag)\n",
    "        logger.info(\"{} new outliers detected\".format(np.sum(false_to_true_mask)))\n",
    "        df_obs.loc[:,\"outlier\"] = outlier_flag\n",
    "        df_obs.loc[false_to_true_mask, ['ssObjectId', 'filter_name', 'diaSourceId', 'midPointMjdTai', 'outlier']].to_sql('AdlerFlags', con=conn_adler_out, if_exists='append', index=False)\n",
    "        logger.info(f\"New flags for {obj_id} written to AdlerFlags table\")\n",
    "\n",
    "        # # Write all flags (True and False)\n",
    "        # df_obs.loc[:,\"outlier\"] = outlier_flag\n",
    "        # df_obs[['ssObjectId', 'filter_name', 'diaSourceId', 'midPointMjdTai', 'outlier']].to_sql('AdlerFlags', con=conn_adler_out, if_exists='append', index=False)\n",
    "        \n",
    "        #Populate nobs parameter\n",
    "        #FIXME Currently using the number of observations in the filter before masking out values with large errorbars\n",
    "        # This is so that the comparison with the new observations file can be made to assess what objects have new observations\n",
    "        # This may need to change (possibly populate a new parameter) as technically this value should be the number of values used in the fit\n",
    "        # But currently the \"fit\" is just taking the median anyway, rather than any kind of phase curve\n",
    "        # Possible solution is in the new observations file loop, we could load in the observations for each object and then just continue to next object if no new observations (doing a check on diaSourceId to guard against if weird stuff happens with the masking)\n",
    "        adler_data.populate_phase_parameters(filt, **{'nobs':nobs_nomask})\n",
    "        adler_data.write_row_to_database(adler_output_filename)\n",
    "        \n",
    "        if make_plots:\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.errorbar(df_obs['midPointMjdTai'], df_obs['mag'], df_obs['magErr'], ls='', marker='.')\n",
    "            ax.errorbar(df_obs[outlier_flag]['midPointMjdTai'], df_obs[outlier_flag]['mag'], df_obs[outlier_flag]['magErr'], ls='', marker='.')\n",
    "            ax.axhline(median_mag)\n",
    "            ax.invert_yaxis()\n",
    "            ax.set_xlabel(\"Time [MJD]\")\n",
    "            ax.set_ylabel(f\"{filt}-band Magnitude\")\n",
    "            fig.savefig(f\"plots/{obj_id}_{filt}_outliers.png\", bbox_inches='tight', pad_inches=0.05)\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0fc2e8",
   "metadata": {},
   "source": [
    "# Specified processing date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0cfd67",
   "metadata": {},
   "source": [
    "### Idea here is to set it so that if a processing date is specified then the last N nights of data are considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbab6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO implement ability to specify a processing dates\n",
    "process_mjd = 60800.5\n",
    "process_date = mjd_to_utc(process_mjd)\n",
    "\n",
    "# Number of days of data to retrieve (i.e. previous 30 nights)\n",
    "data_timespan=30\n",
    "\n",
    "#what is the plan for writing out/reading in in this situation?\n",
    "\n",
    "#should we be considering the model as the previous nights and then check for outliers in the new night?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4247312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it so that new file written to, the idea is that these functions and general functionality can be adapted to whatever actual database is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6e4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b3ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plots=True\n",
    "\n",
    "for obj_id in tqdm(obj_list):\n",
    "    #Set first_analysis flag to False by default (object could have new observations so would still appear in AdlerData/AdlerFlags)\n",
    "    first_analysis = False\n",
    "    #Taking all data, no time constraint\n",
    "    planetoid = AdlerPlanetoid.construct_from_mpc_obs_sbn(ssObjectId=obj_id,\n",
    "                                                        sql_filename=rubin_sql_filename_nov,\n",
    "                                                        filter_list=filter_list,\n",
    "                                                        date_range=[process_mjd-data_timespan, process_mjd])\n",
    "\n",
    "    adler_data = AdlerData(obj_id, planetoid.filter_list)\n",
    "    try:\n",
    "        adler_data.populate_from_database(adler_output_filename)\n",
    "    except ValueError:\n",
    "        logger.info(f\"No AdlerData entry for {obj_id}, proceeding with empty AdlerData object\")\n",
    "        first_analysis = True\n",
    "\n",
    "    for filt in planetoid.filter_list:\n",
    "        df_obs = sci_utils.get_df_obs_filt(planetoid, filt=filt)\n",
    "\n",
    "        nobs_nomask = len(df_obs)\n",
    "\n",
    "        err_flag = df_obs.magErr.isnull().all()\n",
    "        if err_flag:\n",
    "            logger.info(\"All magErr values are NaNs, proceed with caution\")\n",
    "        else:\n",
    "            # Remove observations with large errorbars\n",
    "            magErr_percentile_cut = 95    #Value (between 0 and 100) to define the percentile above which we cut data with large magErr values\n",
    "            magErr_mask = (df_obs.magErr <= np.nanpercentile(df_obs.magErr, q=magErr_percentile_cut))\n",
    "            df_obs = df_obs[magErr_mask]\n",
    "        \n",
    "        # First loop so there are no previous outliers\n",
    "        if first_analysis:\n",
    "            logger.info(\"First analysis set to True so no outlier flags available to load\")\n",
    "            df_obs[\"outlier\"] = [False] * len(df_obs)\n",
    "        else:\n",
    "            logger.info(\"Loading previous outlier flags\")\n",
    "            sql_query = f\"SELECT * FROM AdlerFlags WHERE ssObjectId='{obj_id}' AND filter_name='{filt}'\"\n",
    "            flags_df = pd.read_sql_query(sql_query, conn_adler_out)\n",
    "            logger.info(\"{} previous outliers found\".format(len(flags_df)))\n",
    "\n",
    "            # Include the outlier flags in the observations dataframe\n",
    "            df_obs = df_obs.merge(flags_df[['diaSourceId', 'outlier']],\n",
    "                                on='diaSourceId',\n",
    "                                how='left')\n",
    "            df_obs['outlier'] = df_obs['outlier'].astype('boolean').fillna(False)\n",
    "\n",
    "        \n",
    "        median_mag = np.median(df_obs.loc[~df_obs['outlier'], mag_col])\n",
    "        res = np.array(df_obs[mag_col]) - median_mag\n",
    "        \n",
    "        if err_flag:\n",
    "            logger.info(f\"No measurement errors, using outlier_diff to check for outliers more than {diff_cut} magnitudes away\")\n",
    "            outlier_flag = sci_utils.outlier_diff(res, diff_cut=diff_cut)\n",
    "        else:\n",
    "            logger.info(f\"Measurement errors available, using outlier_sigma_diff to check for outliers more than {std_cut} standard deviation away\")\n",
    "            outlier_flag = sci_utils.outlier_sigma_diff(res, df_obs[magErr_col], std_sigma=std_cut)\n",
    "\n",
    "        logger.info(\"{} outliers detected\".format(np.sum(outlier_flag)))\n",
    "        \n",
    "        # Check if any outliers swapped from True to False and update in AdlerFlags\n",
    "        true_to_false_mask = (df_obs['outlier']) & (~outlier_flag)\n",
    "        logger.info(\"{} previous outliers now determined to be not significant\".format(np.sum(true_to_false_mask)))\n",
    "        if np.sum(true_to_false_mask)>0:\n",
    "            update_query = f\"\"\"UPDATE AdlerFlags SET outlier=0 WHERE diaSourceId IN {df_obs.loc[true_to_false_mask, 'diaSourceId'].to_numpy()}\"\"\"\n",
    "            logger.info(f\"Executing {update_query} on {adler_output_filename}\")\n",
    "            cur_adler_out.execute(update_query)\n",
    "            conn_adler_out.commit()\n",
    "\n",
    "        # Check for new outliers (i.e. have swapped from False to True)\n",
    "        false_to_true_mask = (~df_obs['outlier']) & (outlier_flag)\n",
    "        logger.info(\"{} new outliers detected\".format(np.sum(false_to_true_mask)))\n",
    "        df_obs.loc[:,\"outlier\"] = outlier_flag\n",
    "        df_obs.loc[false_to_true_mask, ['ssObjectId', 'filter_name', 'diaSourceId', 'midPointMjdTai', 'outlier']].to_sql('AdlerFlags', con=conn_adler_out, if_exists='append', index=False)\n",
    "        logger.info(f\"New flags for {obj_id} written to AdlerFlags table\")\n",
    "\n",
    "        # # Write all flags (True and False)\n",
    "        # df_obs.loc[:,\"outlier\"] = outlier_flag\n",
    "        # df_obs[['ssObjectId', 'filter_name', 'diaSourceId', 'midPointMjdTai', 'outlier']].to_sql('AdlerFlags', con=conn_adler_out, if_exists='append', index=False)\n",
    "        \n",
    "        #Populate nobs parameter\n",
    "        #FIXME Currently using the number of observations in the filter before masking out values with large errorbars\n",
    "        # This is so that the comparison with the new observations file can be made to assess what objects have new observations\n",
    "        # This may need to change (possibly populate a new parameter) as technically this value should be the number of values used in the fit\n",
    "        # But currently the \"fit\" is just taking the median anyway, rather than any kind of phase curve\n",
    "        # Possible solution is in the new observations file loop, we could load in the observations for each object and then just continue to next object if no new observations (doing a check on diaSourceId to guard against if weird stuff happens with the masking)\n",
    "        # Does LinearPhaseFunc satisfied our needs? possibly not because it expects absolute magnitude, but maybe that works?\n",
    "        adler_data.populate_phase_parameters(filt, **{'nobs':nobs_nomask})\n",
    "        adler_data.write_row_to_database(adler_output_filename)\n",
    "        \n",
    "        if make_plots:\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.errorbar(df_obs['midPointMjdTai'], df_obs['mag'], df_obs['magErr'], ls='', marker='.')\n",
    "            ax.errorbar(df_obs[outlier_flag]['midPointMjdTai'], df_obs[outlier_flag]['mag'], df_obs[outlier_flag]['magErr'], ls='', marker='.')\n",
    "            ax.axhline(median_mag)\n",
    "            ax.invert_yaxis()\n",
    "            ax.set_xlabel(\"Time [MJD]\")\n",
    "            ax.set_ylabel(f\"{filt}-band Magnitude\")\n",
    "            fig.savefig(f\"plots/{obj_id}_{filt}_outliers.png\", bbox_inches='tight', pad_inches=0.05)\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf86cebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cef9afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>458132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   COUNT(*)\n",
       "0    458132"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql_query(\"SELECT COUNT(*) FROM obs_sbn\", conn_nov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2031582a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adler_plot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
